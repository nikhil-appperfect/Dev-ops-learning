I0731 05:37:52.719922       1 serving.go:386] Generated self-signed cert in-memory
I0731 05:37:53.163847       1 controllermanager.go:188] "Starting" version="v1.33.1"
I0731 05:37:53.163905       1 controllermanager.go:190] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0731 05:37:53.171391       1 secure_serving.go:211] Serving securely on 127.0.0.1:10257
I0731 05:37:53.172876       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0731 05:37:53.172972       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0731 05:37:53.172995       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0731 05:37:58.898455       1 controllermanager.go:778] "Started controller" controller="serviceaccount-token-controller"
I0731 05:37:58.898863       1 shared_informer.go:350] "Waiting for caches to sync" controller="tokens"
I0731 05:37:58.934818       1 controllermanager.go:778] "Started controller" controller="taint-eviction-controller"
I0731 05:37:58.934845       1 controllermanager.go:741] "Warning: controller is disabled" controller="selinux-warning-controller"
I0731 05:37:58.935934       1 taint_eviction.go:282] "Starting" logger="taint-eviction-controller" controller="taint-eviction-controller"
I0731 05:37:58.935980       1 taint_eviction.go:288] "Sending events to api server" logger="taint-eviction-controller"
I0731 05:37:58.936000       1 shared_informer.go:350] "Waiting for caches to sync" controller="taint-eviction-controller"
I0731 05:37:59.102162       1 controllermanager.go:778] "Started controller" controller="job-controller"
I0731 05:37:59.104072       1 shared_informer.go:357] "Caches are synced" controller="tokens"
I0731 05:37:59.106420       1 job_controller.go:243] "Starting job controller" logger="job-controller"
I0731 05:37:59.106438       1 shared_informer.go:350] "Waiting for caches to sync" controller="job"
I0731 05:37:59.164356       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="daemonsets.apps"
I0731 05:37:59.170434       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="ingresses.networking.k8s.io"
I0731 05:37:59.170565       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="endpointslices.discovery.k8s.io"
I0731 05:37:59.170680       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="servicemonitors.monitoring.coreos.com"
I0731 05:37:59.170710       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="endpoints"
I0731 05:37:59.170723       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="statefulsets.apps"
I0731 05:37:59.170787       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="leases.coordination.k8s.io"
I0731 05:37:59.170804       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="controllerrevisions.apps"
I0731 05:37:59.170839       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="horizontalpodautoscalers.autoscaling"
I0731 05:37:59.170853       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="alertmanagers.monitoring.coreos.com"
I0731 05:37:59.170863       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="podmonitors.monitoring.coreos.com"
I0731 05:37:59.170925       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="serviceaccounts"
I0731 05:37:59.180801       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="csistoragecapacities.storage.k8s.io"
I0731 05:37:59.180859       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="networkpolicies.networking.k8s.io"
I0731 05:37:59.180881       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="limitranges"
I0731 05:37:59.180899       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="replicasets.apps"
I0731 05:37:59.180909       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="jobs.batch"
I0731 05:37:59.180921       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="podtemplates"
I0731 05:37:59.186714       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="poddisruptionbudgets.policy"
I0731 05:37:59.186766       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="prometheuses.monitoring.coreos.com"
I0731 05:37:59.186808       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="scrapeconfigs.monitoring.coreos.com"
I0731 05:37:59.186820       1 shared_informer.go:683] "Warning: resync period is smaller than resync check period and the informer has already started. Changing it to the resync check period" resyncPeriod="17h14m48.816914086s" resyncCheckPeriod="18h19m47.175349434s"
I0731 05:37:59.187035       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="cronjobs.batch"
I0731 05:37:59.187097       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="roles.rbac.authorization.k8s.io"
I0731 05:37:59.187124       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="probes.monitoring.coreos.com"
I0731 05:37:59.187139       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="prometheusrules.monitoring.coreos.com"
I0731 05:37:59.187156       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="rolebindings.rbac.authorization.k8s.io"
I0731 05:37:59.187171       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="thanosrulers.monitoring.coreos.com"
I0731 05:37:59.187185       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="alertmanagerconfigs.monitoring.coreos.com"
I0731 05:37:59.187195       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="prometheusagents.monitoring.coreos.com"
I0731 05:37:59.187206       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="deployments.apps"
I0731 05:37:59.187226       1 controllermanager.go:778] "Started controller" controller="resourcequota-controller"
I0731 05:37:59.187827       1 resource_quota_controller.go:300] "Starting resource quota controller" logger="resourcequota-controller"
I0731 05:37:59.187850       1 shared_informer.go:350] "Waiting for caches to sync" controller="resource quota"
I0731 05:37:59.187884       1 resource_quota_monitor.go:308] "QuotaMonitor running" logger="resourcequota-controller"
I0731 05:37:59.197952       1 controllermanager.go:778] "Started controller" controller="statefulset-controller"
I0731 05:37:59.198232       1 stateful_set.go:166] "Starting stateful set controller" logger="statefulset-controller"
I0731 05:37:59.198250       1 shared_informer.go:350] "Waiting for caches to sync" controller="stateful set"
I0731 05:37:59.210586       1 controllermanager.go:778] "Started controller" controller="certificatesigningrequest-signing-controller"
I0731 05:37:59.211941       1 certificate_controller.go:120] "Starting certificate controller" logger="certificatesigningrequest-signing-controller" name="csrsigning-kubelet-serving"
I0731 05:37:59.211995       1 shared_informer.go:350] "Waiting for caches to sync" controller="certificate-csrsigning-kubelet-serving"
I0731 05:37:59.212037       1 certificate_controller.go:120] "Starting certificate controller" logger="certificatesigningrequest-signing-controller" name="csrsigning-kubelet-client"
I0731 05:37:59.212055       1 shared_informer.go:350] "Waiting for caches to sync" controller="certificate-csrsigning-kubelet-client"
I0731 05:37:59.212088       1 certificate_controller.go:120] "Starting certificate controller" logger="certificatesigningrequest-signing-controller" name="csrsigning-kube-apiserver-client"
I0731 05:37:59.212107       1 shared_informer.go:350] "Waiting for caches to sync" controller="certificate-csrsigning-kube-apiserver-client"
I0731 05:37:59.212129       1 certificate_controller.go:120] "Starting certificate controller" logger="certificatesigningrequest-signing-controller" name="csrsigning-legacy-unknown"
I0731 05:37:59.212134       1 shared_informer.go:350] "Waiting for caches to sync" controller="certificate-csrsigning-legacy-unknown"
I0731 05:37:59.212147       1 dynamic_serving_content.go:135] "Starting controller" name="csr-controller::/var/lib/minikube/certs/ca.crt::/var/lib/minikube/certs/ca.key"
I0731 05:37:59.212234       1 dynamic_serving_content.go:135] "Starting controller" name="csr-controller::/var/lib/minikube/certs/ca.crt::/var/lib/minikube/certs/ca.key"
I0731 05:37:59.212269       1 dynamic_serving_content.go:135] "Starting controller" name="csr-controller::/var/lib/minikube/certs/ca.crt::/var/lib/minikube/certs/ca.key"
I0731 05:37:59.212299       1 dynamic_serving_content.go:135] "Starting controller" name="csr-controller::/var/lib/minikube/certs/ca.crt::/var/lib/minikube/certs/ca.key"
I0731 05:37:59.216537       1 controllermanager.go:778] "Started controller" controller="certificatesigningrequest-approving-controller"
I0731 05:37:59.218096       1 certificate_controller.go:120] "Starting certificate controller" logger="certificatesigningrequest-approving-controller" name="csrapproving"
I0731 05:37:59.218124       1 shared_informer.go:350] "Waiting for caches to sync" controller="certificate-csrapproving"
I0731 05:37:59.253679       1 node_lifecycle_controller.go:419] "Controller will reconcile labels" logger="node-lifecycle-controller"
I0731 05:37:59.253859       1 controllermanager.go:778] "Started controller" controller="node-lifecycle-controller"
I0731 05:37:59.253871       1 controllermanager.go:736] "Skipping a cloud provider controller" controller="cloud-node-lifecycle-controller"
I0731 05:37:59.255304       1 node_lifecycle_controller.go:453] "Sending events to api server" logger="node-lifecycle-controller"
I0731 05:37:59.255335       1 node_lifecycle_controller.go:464] "Starting node controller" logger="node-lifecycle-controller"
I0731 05:37:59.255342       1 shared_informer.go:350] "Waiting for caches to sync" controller="taint"
I0731 05:37:59.262112       1 controllermanager.go:778] "Started controller" controller="pod-garbage-collector-controller"
I0731 05:37:59.265251       1 gc_controller.go:99] "Starting GC controller" logger="pod-garbage-collector-controller"
I0731 05:37:59.265283       1 shared_informer.go:350] "Waiting for caches to sync" controller="GC"
I0731 05:37:59.274710       1 controllermanager.go:778] "Started controller" controller="cronjob-controller"
I0731 05:37:59.276097       1 cronjob_controllerv2.go:145] "Starting cronjob controller v2" logger="cronjob-controller"
I0731 05:37:59.276118       1 shared_informer.go:350] "Waiting for caches to sync" controller="cronjob"
I0731 05:37:59.279619       1 controllermanager.go:778] "Started controller" controller="certificatesigningrequest-cleaner-controller"
I0731 05:37:59.284824       1 cleaner.go:83] "Starting CSR cleaner controller" logger="certificatesigningrequest-cleaner-controller"
I0731 05:37:59.289311       1 controllermanager.go:778] "Started controller" controller="ephemeral-volume-controller"
I0731 05:37:59.289325       1 controllermanager.go:730] "Controller is disabled by a feature gate" controller="storageversion-garbage-collector-controller" requiredFeatureGates=["APIServerIdentity","StorageVersionAPI"]
I0731 05:37:59.289349       1 controllermanager.go:730] "Controller is disabled by a feature gate" controller="device-taint-eviction-controller" requiredFeatureGates=["DynamicResourceAllocation","DRADeviceTaints"]
I0731 05:37:59.290825       1 controller.go:173] "Starting ephemeral volume controller" logger="ephemeral-volume-controller"
I0731 05:37:59.290865       1 shared_informer.go:350] "Waiting for caches to sync" controller="ephemeral"
I0731 05:37:59.292372       1 controllermanager.go:778] "Started controller" controller="replicaset-controller"
I0731 05:37:59.293226       1 replica_set.go:219] "Starting controller" logger="replicaset-controller" name="replicaset"
I0731 05:37:59.293246       1 shared_informer.go:350] "Waiting for caches to sync" controller="ReplicaSet"
I0731 05:37:59.297138       1 controllermanager.go:778] "Started controller" controller="endpointslice-mirroring-controller"
I0731 05:37:59.299212       1 endpointslicemirroring_controller.go:227] "Starting EndpointSliceMirroring controller" logger="endpointslice-mirroring-controller"
I0731 05:37:59.299246       1 shared_informer.go:350] "Waiting for caches to sync" controller="endpoint_slice_mirroring"
I0731 05:37:59.313411       1 controllermanager.go:778] "Started controller" controller="serviceaccount-controller"
I0731 05:37:59.314296       1 serviceaccounts_controller.go:114] "Starting service account controller" logger="serviceaccount-controller"
I0731 05:37:59.314310       1 shared_informer.go:350] "Waiting for caches to sync" controller="service account"
I0731 05:37:59.320229       1 controllermanager.go:778] "Started controller" controller="clusterrole-aggregation-controller"
I0731 05:37:59.320246       1 controllermanager.go:730] "Controller is disabled by a feature gate" controller="volumeattributesclass-protection-controller" requiredFeatureGates=["VolumeAttributesClass"]
I0731 05:37:59.326529       1 clusterroleaggregation_controller.go:194] "Starting ClusterRoleAggregator controller" logger="clusterrole-aggregation-controller"
I0731 05:37:59.326565       1 shared_informer.go:350] "Waiting for caches to sync" controller="ClusterRoleAggregator"
I0731 05:37:59.332610       1 controllermanager.go:778] "Started controller" controller="ttl-after-finished-controller"
I0731 05:37:59.336958       1 controllermanager.go:730] "Controller is disabled by a feature gate" controller="resourceclaim-controller" requiredFeatureGates=["DynamicResourceAllocation"]
I0731 05:37:59.336536       1 ttlafterfinished_controller.go:112] "Starting TTL after finished controller" logger="ttl-after-finished-controller"
I0731 05:37:59.338018       1 shared_informer.go:350] "Waiting for caches to sync" controller="TTL after finished"
I0731 05:37:59.365363       1 garbagecollector.go:144] "Starting controller" logger="garbage-collector-controller" controller="garbagecollector"
I0731 05:37:59.365391       1 shared_informer.go:350] "Waiting for caches to sync" controller="garbage collector"
I0731 05:37:59.365414       1 graph_builder.go:351] "Running" logger="garbage-collector-controller" component="GraphBuilder"
I0731 05:37:59.365593       1 controllermanager.go:778] "Started controller" controller="garbage-collector-controller"
I0731 05:37:59.426147       1 controllermanager.go:778] "Started controller" controller="horizontal-pod-autoscaler-controller"
I0731 05:37:59.426419       1 horizontal.go:204] "Starting HPA controller" logger="horizontal-pod-autoscaler-controller"
I0731 05:37:59.426430       1 shared_informer.go:350] "Waiting for caches to sync" controller="HPA"
I0731 05:37:59.438275       1 controllermanager.go:778] "Started controller" controller="disruption-controller"
I0731 05:37:59.438436       1 disruption.go:455] "Sending events to api server." logger="disruption-controller"
I0731 05:37:59.438481       1 disruption.go:466] "Starting disruption controller" logger="disruption-controller"
I0731 05:37:59.438489       1 shared_informer.go:350] "Waiting for caches to sync" controller="disruption"
I0731 05:37:59.463360       1 controllermanager.go:778] "Started controller" controller="bootstrap-signer-controller"
I0731 05:37:59.463611       1 shared_informer.go:350] "Waiting for caches to sync" controller="bootstrap_signer"
I0731 05:37:59.519238       1 range_allocator.go:112] "No Secondary Service CIDR provided. Skipping filtering out secondary service addresses" logger="node-ipam-controller"
I0731 05:37:59.520382       1 controllermanager.go:778] "Started controller" controller="node-ipam-controller"
I0731 05:37:59.520423       1 controllermanager.go:736] "Skipping a cloud provider controller" controller="service-lb-controller"
I0731 05:37:59.520795       1 node_ipam_controller.go:141] "Starting ipam controller" logger="node-ipam-controller"
I0731 05:37:59.521025       1 shared_informer.go:350] "Waiting for caches to sync" controller="node"
I0731 05:37:59.555199       1 controllermanager.go:778] "Started controller" controller="root-ca-certificate-publisher-controller"
I0731 05:37:59.555424       1 publisher.go:107] "Starting root CA cert publisher controller" logger="root-ca-certificate-publisher-controller"
I0731 05:37:59.555434       1 shared_informer.go:350] "Waiting for caches to sync" controller="crt configmap"
I0731 05:37:59.573209       1 controllermanager.go:778] "Started controller" controller="daemonset-controller"
I0731 05:37:59.573551       1 daemon_controller.go:310] "Starting daemon sets controller" logger="daemonset-controller"
I0731 05:37:59.573566       1 shared_informer.go:350] "Waiting for caches to sync" controller="daemon sets"
I0731 05:37:59.583129       1 controllermanager.go:778] "Started controller" controller="persistentvolumeclaim-protection-controller"
I0731 05:37:59.584198       1 pvc_protection_controller.go:168] "Starting PVC protection controller" logger="persistentvolumeclaim-protection-controller"
I0731 05:37:59.584221       1 shared_informer.go:350] "Waiting for caches to sync" controller="PVC protection"
I0731 05:37:59.589654       1 controllermanager.go:778] "Started controller" controller="persistentvolume-protection-controller"
I0731 05:37:59.591885       1 pv_protection_controller.go:81] "Starting PV protection controller" logger="persistentvolume-protection-controller"
I0731 05:37:59.591916       1 shared_informer.go:350] "Waiting for caches to sync" controller="PV protection"
I0731 05:37:59.594333       1 controllermanager.go:778] "Started controller" controller="service-cidr-controller"
I0731 05:37:59.594988       1 servicecidrs_controller.go:136] "Starting" logger="service-cidr-controller" controller="service-cidr-controller"
I0731 05:37:59.595614       1 shared_informer.go:350] "Waiting for caches to sync" controller="service-cidr-controller"
I0731 05:37:59.598276       1 controllermanager.go:778] "Started controller" controller="endpointslice-controller"
I0731 05:37:59.599511       1 endpointslice_controller.go:281] "Starting endpoint slice controller" logger="endpointslice-controller"
I0731 05:37:59.599554       1 shared_informer.go:350] "Waiting for caches to sync" controller="endpoint_slice"
I0731 05:37:59.602628       1 controllermanager.go:778] "Started controller" controller="token-cleaner-controller"
I0731 05:37:59.602966       1 controllermanager.go:736] "Skipping a cloud provider controller" controller="node-route-controller"
I0731 05:37:59.602914       1 tokencleaner.go:117] "Starting token cleaner controller" logger="token-cleaner-controller"
I0731 05:37:59.604592       1 shared_informer.go:350] "Waiting for caches to sync" controller="token_cleaner"
I0731 05:37:59.604604       1 shared_informer.go:357] "Caches are synced" controller="token_cleaner"
I0731 05:37:59.608593       1 controllermanager.go:778] "Started controller" controller="persistentvolume-binder-controller"
I0731 05:37:59.608622       1 controllermanager.go:756] "Warning: skipping controller" controller="storage-version-migrator-controller"
I0731 05:37:59.608705       1 pv_controller_base.go:308] "Starting persistent volume controller" logger="persistentvolume-binder-controller"
I0731 05:37:59.608715       1 shared_informer.go:350] "Waiting for caches to sync" controller="persistent volume"
I0731 05:37:59.666454       1 controllermanager.go:778] "Started controller" controller="deployment-controller"
I0731 05:37:59.666551       1 deployment_controller.go:173] "Starting controller" logger="deployment-controller" controller="deployment"
I0731 05:37:59.666561       1 shared_informer.go:350] "Waiting for caches to sync" controller="deployment"
I0731 05:37:59.713021       1 controllermanager.go:778] "Started controller" controller="endpoints-controller"
I0731 05:37:59.713103       1 endpoints_controller.go:187] "Starting endpoint controller" logger="endpoints-controller"
I0731 05:37:59.713111       1 shared_informer.go:350] "Waiting for caches to sync" controller="endpoint"
I0731 05:37:59.760357       1 controllermanager.go:778] "Started controller" controller="replicationcontroller-controller"
I0731 05:37:59.760454       1 replica_set.go:219] "Starting controller" logger="replicationcontroller-controller" name="replicationcontroller"
I0731 05:37:59.760554       1 shared_informer.go:350] "Waiting for caches to sync" controller="ReplicationController"
I0731 05:37:59.927496       1 controllermanager.go:778] "Started controller" controller="namespace-controller"
I0731 05:37:59.927897       1 namespace_controller.go:202] "Starting namespace controller" logger="namespace-controller"
I0731 05:37:59.927966       1 shared_informer.go:350] "Waiting for caches to sync" controller="namespace"
I0731 05:37:59.960316       1 controllermanager.go:778] "Started controller" controller="ttl-controller"
I0731 05:37:59.960494       1 ttl_controller.go:127] "Starting TTL controller" logger="ttl-controller"
I0731 05:37:59.960512       1 shared_informer.go:350] "Waiting for caches to sync" controller="TTL"
I0731 05:38:00.027854       1 controllermanager.go:778] "Started controller" controller="persistentvolume-attach-detach-controller"
I0731 05:38:00.027970       1 attach_detach_controller.go:338] "Starting attach detach controller" logger="persistentvolume-attach-detach-controller"
I0731 05:38:00.027979       1 shared_informer.go:350] "Waiting for caches to sync" controller="attach detach"
I0731 05:38:00.063139       1 controllermanager.go:778] "Started controller" controller="persistentvolume-expander-controller"
I0731 05:38:00.063220       1 expand_controller.go:329] "Starting expand controller" logger="persistentvolume-expander-controller"
I0731 05:38:00.063238       1 shared_informer.go:350] "Waiting for caches to sync" controller="expand"
I0731 05:38:00.110353       1 controllermanager.go:778] "Started controller" controller="legacy-serviceaccount-token-cleaner-controller"
I0731 05:38:00.110402       1 controllermanager.go:730] "Controller is disabled by a feature gate" controller="kube-apiserver-serving-clustertrustbundle-publisher-controller" requiredFeatureGates=["ClusterTrustBundle"]
I0731 05:38:00.113164       1 legacy_serviceaccount_token_cleaner.go:103] "Starting legacy service account token cleaner controller" logger="legacy-serviceaccount-token-cleaner-controller"
I0731 05:38:00.113860       1 shared_informer.go:350] "Waiting for caches to sync" controller="legacy-service-account-token-cleaner"
I0731 05:38:00.214204       1 controllermanager.go:778] "Started controller" controller="validatingadmissionpolicy-status-controller"
I0731 05:38:00.217764       1 shared_informer.go:350] "Waiting for caches to sync" controller="validatingadmissionpolicy-status"
I0731 05:38:00.270493       1 shared_informer.go:350] "Waiting for caches to sync" controller="resource quota"
I0731 05:38:00.295976       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0731 05:38:00.322279       1 shared_informer.go:357] "Caches are synced" controller="node"
I0731 05:38:00.322352       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0731 05:38:00.322384       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0731 05:38:00.322391       1 shared_informer.go:350] "Waiting for caches to sync" controller="cidrallocator"
I0731 05:38:00.322396       1 shared_informer.go:357] "Caches are synced" controller="cidrallocator"
I0731 05:38:00.387122       1 shared_informer.go:357] "Caches are synced" controller="TTL"
I0731 05:38:00.393505       1 shared_informer.go:357] "Caches are synced" controller="PV protection"
I0731 05:38:00.393562       1 shared_informer.go:357] "Caches are synced" controller="ReplicaSet"
I0731 05:38:00.409449       1 shared_informer.go:357] "Caches are synced" controller="service-cidr-controller"
I0731 05:38:00.411128       1 shared_informer.go:357] "Caches are synced" controller="job"
I0731 05:38:00.413562       1 shared_informer.go:357] "Caches are synced" controller="endpoint"
I0731 05:38:00.414190       1 shared_informer.go:357] "Caches are synced" controller="legacy-service-account-token-cleaner"
I0731 05:38:00.414942       1 shared_informer.go:357] "Caches are synced" controller="service account"
I0731 05:38:00.417940       1 shared_informer.go:357] "Caches are synced" controller="validatingadmissionpolicy-status"
I0731 05:38:00.454299       1 shared_informer.go:357] "Caches are synced" controller="TTL after finished"
I0731 05:38:00.454334       1 shared_informer.go:357] "Caches are synced" controller="namespace"
I0731 05:38:00.454459       1 shared_informer.go:357] "Caches are synced" controller="taint-eviction-controller"
I0731 05:38:00.456164       1 shared_informer.go:357] "Caches are synced" controller="taint"
I0731 05:38:00.456271       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0731 05:38:00.456359       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0731 05:38:00.456408       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0731 05:38:00.459001       1 shared_informer.go:350] "Waiting for caches to sync" controller="garbage collector"
I0731 05:38:00.459127       1 shared_informer.go:357] "Caches are synced" controller="crt configmap"
I0731 05:38:00.463313       1 shared_informer.go:357] "Caches are synced" controller="expand"
I0731 05:38:00.465014       1 shared_informer.go:357] "Caches are synced" controller="bootstrap_signer"
I0731 05:38:00.466132       1 shared_informer.go:357] "Caches are synced" controller="GC"
I0731 05:38:00.467740       1 shared_informer.go:357] "Caches are synced" controller="deployment"
I0731 05:38:00.469923       1 shared_informer.go:357] "Caches are synced" controller="ReplicationController"
I0731 05:38:00.470598       1 shared_informer.go:357] "Caches are synced" controller="ClusterRoleAggregator"
I0731 05:38:00.479076       1 shared_informer.go:357] "Caches are synced" controller="cronjob"
I0731 05:38:00.480047       1 shared_informer.go:357] "Caches are synced" controller="daemon sets"
I0731 05:38:00.484858       1 shared_informer.go:357] "Caches are synced" controller="PVC protection"
I0731 05:38:00.491202       1 shared_informer.go:357] "Caches are synced" controller="ephemeral"
I0731 05:38:00.498449       1 shared_informer.go:357] "Caches are synced" controller="stateful set"
I0731 05:38:00.501809       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice_mirroring"
I0731 05:38:00.505666       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice"
I0731 05:38:00.512172       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I0731 05:38:00.514458       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I0731 05:38:00.514527       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I0731 05:38:00.514565       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I0731 05:38:00.556611       1 shared_informer.go:357] "Caches are synced" controller="attach detach"
I0731 05:38:00.557177       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrapproving"
I0731 05:38:00.661893       1 shared_informer.go:357] "Caches are synced" controller="disruption"
I0731 05:38:00.710606       1 shared_informer.go:357] "Caches are synced" controller="persistent volume"
I0731 05:38:00.754480       1 shared_informer.go:357] "Caches are synced" controller="HPA"
I0731 05:38:02.460192       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0731 05:38:02.467170       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0731 05:38:02.467191       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0731 05:38:02.467198       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0731 05:38:02.478073       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0731 05:38:02.502231       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0731 05:48:04.964849       1 namespace_controller.go:187] "Namespace has been deleted" logger="namespace-controller" namespace="demo-namespace"
E0731 06:02:18.997660       1 pvc_protection_controller.go:291] "Error removing protection finalizer from PVC" err="Operation cannot be fulfilled on persistentvolumeclaims \"app-pvc-75c2040a\": the object has been modified; please apply your changes to the latest version and try again" logger="persistentvolumeclaim-protection-controller" PVC="default/app-pvc-75c2040a"
E0731 06:02:18.999836       1 pvc_protection_controller.go:225] "Unhandled Error" err="PVC app-pvc-75c2040a/default failed with: Operation cannot be fulfilled on persistentvolumeclaims \"app-pvc-75c2040a\": the object has been modified; please apply your changes to the latest version and try again" logger="UnhandledError"
I0731 06:02:30.739462       1 namespace_controller.go:187] "Namespace has been deleted" logger="namespace-controller" namespace="demo-namespace"
I0731 08:02:46.687665       1 node_lifecycle_controller.go:1025] "Controller detected that all Nodes are not-Ready. Entering master disruption mode" logger="node-lifecycle-controller"
I0731 08:18:45.371252       1 request.go:752] "Waited before sending request" logger="persistentvolume-binder-controller" delay="1.029702614s" reason="client-side throttling, not priority and fairness" verb="GET" URL="https://192.168.49.2:8443/api/v1/persistentvolumes/pvc-d2c7adee-0946-4c7e-8893-de30b07c2eb5"
I0731 08:18:49.261839       1 endpointslice_controller.go:344] "Error syncing endpoint slices for service, retrying" logger="endpointslice-controller" key="default/prometheus-operated" err="failed to update prometheus-operated-xmcmb EndpointSlice for Service default/prometheus-operated: etcdserver: request timed out"
I0731 08:18:49.401515       1 event.go:377] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"prometheus-operated", UID:"abcdcb98-3e7d-4da4-9de1-3f4bfab1a11a", APIVersion:"v1", ResourceVersion:"53665", FieldPath:""}): type: 'Warning' reason: 'FailedToUpdateEndpointSlices' Error updating Endpoint Slices for Service default/prometheus-operated: failed to update prometheus-operated-xmcmb EndpointSlice for Service default/prometheus-operated: etcdserver: request timed out
E0731 08:18:49.519286       1 event.go:359] "Server rejected event (will not retry!)" err="Unauthorized" logger="persistentvolume-binder-controller" event="&Event{ObjectMeta:{alertmanager-prometheus-kube-prometheus-alertmanager-db-alertmanager-prometheus-kube-prometheus-alertmanager-0.18573f647d073692  default   66329 0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:PersistentVolumeClaim,Namespace:default,Name:alertmanager-prometheus-kube-prometheus-alertmanager-db-alertmanager-prometheus-kube-prometheus-alertmanager-0,UID:694c1efd-4492-4ffa-938b-80dd19402375,APIVersion:v1,ResourceVersion:52592,FieldPath:,},Reason:FailedBinding,Message:no persistent volumes available for this claim and no storage class is set,Source:EventSource{Component:persistentvolume-controller,Host:,},FirstTimestamp:2025-07-31 05:38:00 +0000 UTC,LastTimestamp:2025-07-31 08:18:43.878632739 +0000 UTC m=+6912.941523081,Count:461,Type:Normal,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:persistentvolume-controller,ReportingInstance:,}"
E0731 08:18:56.936212       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I0731 08:18:59.296473       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E0731 08:19:04.352396       1 event.go:359] "Server rejected event (will not retry!)" err="etcdserver: request timed out" logger="endpointslice-controller" event="&Event{ObjectMeta:{prometheus-operated.1857482aefb44e45  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Service,Namespace:default,Name:prometheus-operated,UID:abcdcb98-3e7d-4da4-9de1-3f4bfab1a11a,APIVersion:v1,ResourceVersion:53665,FieldPath:,},Reason:FailedToUpdateEndpointSlices,Message:Error updating Endpoint Slices for Service default/prometheus-operated: failed to update prometheus-operated-xmcmb EndpointSlice for Service default/prometheus-operated: etcdserver: request timed out,Source:EventSource{Component:endpoint-slice-controller,Host:,},FirstTimestamp:2025-07-31 08:18:49.131494981 +0000 UTC m=+6918.194385319,LastTimestamp:2025-07-31 08:18:49.131494981 +0000 UTC m=+6918.194385319,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:endpoint-slice-controller,ReportingInstance:,}"
I0731 08:19:29.858659       1 endpointslice_controller.go:344] "Error syncing endpoint slices for service, retrying" logger="endpointslice-controller" key="default/prometheus-operated" err="failed to update prometheus-operated-xmcmb EndpointSlice for Service default/prometheus-operated: Timeout: request did not complete within requested timeout - context deadline exceeded"
I0731 08:19:29.767532       1 event.go:377] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"prometheus-operated", UID:"abcdcb98-3e7d-4da4-9de1-3f4bfab1a11a", APIVersion:"v1", ResourceVersion:"53665", FieldPath:""}): type: 'Warning' reason: 'FailedToUpdateEndpointSlices' Error updating Endpoint Slices for Service default/prometheus-operated: failed to update prometheus-operated-xmcmb EndpointSlice for Service default/prometheus-operated: Timeout: request did not complete within requested timeout - context deadline exceeded
E0731 08:19:47.503746       1 node_lifecycle_controller.go:967] "Error updating node" err="etcdserver: request timed out" logger="node-lifecycle-controller" node="minikube"
E0731 08:19:47.594265       1 event.go:359] "Server rejected event (will not retry!)" err="etcdserver: request timed out" logger="endpointslice-controller" event="&Event{ObjectMeta:{prometheus-operated.1857483460fefe02  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Service,Namespace:default,Name:prometheus-operated,UID:abcdcb98-3e7d-4da4-9de1-3f4bfab1a11a,APIVersion:v1,ResourceVersion:53665,FieldPath:,},Reason:FailedToUpdateEndpointSlices,Message:Error updating Endpoint Slices for Service default/prometheus-operated: failed to update prometheus-operated-xmcmb EndpointSlice for Service default/prometheus-operated: Timeout: request did not complete within requested timeout - context deadline exceeded,Source:EventSource{Component:endpoint-slice-controller,Host:,},FirstTimestamp:2025-07-31 08:19:29.686920706 +0000 UTC m=+6958.753354509,LastTimestamp:2025-07-31 08:19:29.686920706 +0000 UTC m=+6958.753354509,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:endpoint-slice-controller,ReportingInstance:,}"
W0731 08:19:51.850687       1 client_builder_dynamic.go:197] get or create service account failed: the server was unable to return a response in the time allotted, but may still be processing the request (get serviceaccounts persistent-volume-binder)
I0731 08:20:09.094628       1 event.go:377] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"prometheus-operated", UID:"abcdcb98-3e7d-4da4-9de1-3f4bfab1a11a", APIVersion:"v1", ResourceVersion:"53665", FieldPath:""}): type: 'Warning' reason: 'FailedToUpdateEndpointSlices' Error updating Endpoint Slices for Service default/prometheus-operated: failed to update prometheus-operated-xmcmb EndpointSlice for Service default/prometheus-operated: Timeout: request did not complete within requested timeout - context deadline exceeded
I0731 08:20:09.616178       1 endpointslice_controller.go:344] "Error syncing endpoint slices for service, retrying" logger="endpointslice-controller" key="default/prometheus-operated" err="failed to update prometheus-operated-xmcmb EndpointSlice for Service default/prometheus-operated: Timeout: request did not complete within requested timeout - context deadline exceeded"
W0731 08:20:30.245388       1 client_builder_dynamic.go:197] get or create service account failed: the server was unable to return a response in the time allotted, but may still be processing the request (get serviceaccounts generic-garbage-collector)
W0731 08:20:30.238567       1 client_builder_dynamic.go:197] get or create service account failed: the server was unable to return a response in the time allotted, but may still be processing the request (get serviceaccounts resourcequota-controller)
I0731 08:21:05.304798       1 node_lifecycle_controller.go:1044] "Controller detected that some Nodes are Ready. Exiting master disruption mode" logger="node-lifecycle-controller"
I0731 08:32:09.137844       1 endpointslice_controller.go:344] "Error syncing endpoint slices for service, retrying" logger="endpointslice-controller" key="default/loki" err="failed to update loki-hfdpr EndpointSlice for Service default/loki: Put \"https://192.168.49.2:8443/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/loki-hfdpr\": dial tcp 192.168.49.2:8443: connect: connection refused"
E0731 08:32:09.143291       1 stateful_set.go:438] "Unhandled Error" err="error syncing StatefulSet default/loki, requeuing: Put \"https://192.168.49.2:8443/apis/apps/v1/namespaces/default/statefulsets/loki/status\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
I0731 08:32:09.180980       1 endpointslice_controller.go:344] "Error syncing endpoint slices for service, retrying" logger="endpointslice-controller" key="default/loki-memberlist" err="failed to update loki-memberlist-lzsjt EndpointSlice for Service default/loki-memberlist: Put \"https://192.168.49.2:8443/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/loki-memberlist-lzsjt\": dial tcp 192.168.49.2:8443: connect: connection refused"
E0731 08:32:09.192486       1 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://192.168.49.2:8443/api/v1/namespaces/default/events\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="taint-eviction-controller" event="&Event{ObjectMeta:{prometheus-kube-prometheus-operator-d89fb8945-f7n5z.1857484a79571fc4  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Pod,Namespace:default,Name:prometheus-kube-prometheus-operator-d89fb8945-f7n5z,UID:,APIVersion:v1,ResourceVersion:,FieldPath:,},Reason:TaintManagerEviction,Message:Cancelling deletion of Pod default/prometheus-kube-prometheus-operator-d89fb8945-f7n5z,Source:EventSource{Component:taint-eviction-controller,Host:,},FirstTimestamp:2025-07-31 08:21:04.584630212 +0000 UTC m=+7053.646674505,LastTimestamp:2025-07-31 08:21:04.584630212 +0000 UTC m=+7053.646674505,Count:1,Type:Normal,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:taint-eviction-controller,ReportingInstance:,}"
I0731 08:32:09.199016       1 endpointslice_controller.go:344] "Error syncing endpoint slices for service, retrying" logger="endpointslice-controller" key="default/loki-headless" err="failed to update loki-headless-fhrrh EndpointSlice for Service default/loki-headless: Put \"https://192.168.49.2:8443/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/loki-headless-fhrrh\": dial tcp 192.168.49.2:8443: connect: connection refused"
E0731 08:32:09.199431       1 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://192.168.49.2:8443/api/v1/namespaces/default/events\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="endpointslice-controller" event="&Event{ObjectMeta:{loki.185748e52b4c6f08  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Service,Namespace:default,Name:loki,UID:b93db4e7-767f-43f6-a0a2-75aa1914dc98,APIVersion:v1,ResourceVersion:49821,FieldPath:,},Reason:FailedToUpdateEndpointSlices,Message:Error updating Endpoint Slices for Service default/loki: failed to update loki-hfdpr EndpointSlice for Service default/loki: Put \"https://192.168.49.2:8443/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/loki-hfdpr\": dial tcp 192.168.49.2:8443: connect: connection refused,Source:EventSource{Component:endpoint-slice-controller,Host:,},FirstTimestamp:2025-07-31 08:32:08.99523764 +0000 UTC m=+7059.472626716,LastTimestamp:2025-07-31 08:32:08.99523764 +0000 UTC m=+7059.472626716,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:endpoint-slice-controller,ReportingInstance:,}"
E0731 08:32:09.164259       1 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://192.168.49.2:8443/api/v1/namespaces/default/events\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="endpoints-controller" event="&Event{ObjectMeta:{loki.185748e52442fdf0  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Endpoints,Namespace:default,Name:loki,UID:9f6b9bd2-cdf1-4cd7-8243-ba9717061d4a,APIVersion:v1,ResourceVersion:66661,FieldPath:,},Reason:FailedToUpdateEndpoint,Message:Failed to update endpoint default/loki: Put \"https://192.168.49.2:8443/api/v1/namespaces/default/endpoints/loki\": dial tcp 192.168.49.2:8443: connect: connection refused,Source:EventSource{Component:endpoint-controller,Host:,},FirstTimestamp:2025-07-31 08:32:08.877178352 +0000 UTC m=+7059.354567437,LastTimestamp:2025-07-31 08:32:08.877178352 +0000 UTC m=+7059.354567437,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:endpoint-controller,ReportingInstance:,}"
I0731 08:32:09.199631       1 event.go:377] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"loki", UID:"b93db4e7-767f-43f6-a0a2-75aa1914dc98", APIVersion:"v1", ResourceVersion:"49821", FieldPath:""}): type: 'Warning' reason: 'FailedToUpdateEndpointSlices' Error updating Endpoint Slices for Service default/loki: failed to update loki-hfdpr EndpointSlice for Service default/loki: Put "https://192.168.49.2:8443/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/loki-hfdpr": dial tcp 192.168.49.2:8443: connect: connection refused
I0731 08:32:09.325802       1 event.go:377] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"loki-memberlist", UID:"c1320576-de64-486f-ae3b-f926cc65b36a", APIVersion:"v1", ResourceVersion:"49814", FieldPath:""}): type: 'Warning' reason: 'FailedToUpdateEndpointSlices' Error updating Endpoint Slices for Service default/loki-memberlist: failed to update loki-memberlist-lzsjt EndpointSlice for Service default/loki-memberlist: Put "https://192.168.49.2:8443/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/loki-memberlist-lzsjt": dial tcp 192.168.49.2:8443: connect: connection refused
I0731 08:32:09.325829       1 event.go:377] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"loki-headless", UID:"1daa4723-0bbe-45bd-87df-97946d1a5c14", APIVersion:"v1", ResourceVersion:"49813", FieldPath:""}): type: 'Warning' reason: 'FailedToUpdateEndpointSlices' Error updating Endpoint Slices for Service default/loki-headless: failed to update loki-headless-fhrrh EndpointSlice for Service default/loki-headless: Put "https://192.168.49.2:8443/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/loki-headless-fhrrh": dial tcp 192.168.49.2:8443: connect: connection refused
E0731 08:32:09.380286       1 stateful_set.go:438] "Unhandled Error" err="error syncing StatefulSet default/loki, requeuing: Put \"https://192.168.49.2:8443/apis/apps/v1/namespaces/default/statefulsets/loki/status\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
E0731 08:32:09.616036       1 stateful_set.go:438] "Unhandled Error" err="error syncing StatefulSet default/loki, requeuing: Put \"https://192.168.49.2:8443/apis/apps/v1/namespaces/default/statefulsets/loki/status\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
E0731 08:32:09.817422       1 stateful_set.go:438] "Unhandled Error" err="error syncing StatefulSet default/loki, requeuing: Put \"https://192.168.49.2:8443/apis/apps/v1/namespaces/default/statefulsets/loki/status\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
E0731 08:32:09.826065       1 controller_utils.go:215] "Unhandled Error" err="unable to remove [&Taint{Key:node.kubernetes.io/not-ready,Value:,Effect:NoSchedule,TimeAdded:2025-07-31 08:02:44 +0000 UTC,} &Taint{Key:node.kubernetes.io/unreachable,Value:,Effect:NoSchedule,TimeAdded:2025-07-31 08:20:42 +0000 UTC,}] unneeded taint from unresponsive Node \"minikube\": Get \"https://192.168.49.2:8443/api/v1/nodes/minikube?resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
E0731 08:32:09.861193       1 node_lifecycle_controller.go:514] "Failed to taint NoSchedule on node, requeue it" err="failed to swap taints of node &Node{ObjectMeta:{minikube    83d775f7-10dc-437d-8d32-ae49037a8a55 66666 0 2025-07-16 09:18:26 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/os:linux kubernetes.io/arch:amd64 kubernetes.io/hostname:minikube kubernetes.io/os:linux minikube.k8s.io/commit:f8f52f5de11fc6ad8244afac475e1d0f96841df1-dirty minikube.k8s.io/name:minikube minikube.k8s.io/primary:true minikube.k8s.io/updated_at:2025_07_16T14_48_30_0700 minikube.k8s.io/version:v1.36.0 node-role.kubernetes.io/control-plane: node.kubernetes.io/exclude-from-external-load-balancers:] map[kubeadm.alpha.kubernetes.io/cri-socket:unix:///var/run/cri-dockerd.sock node.alpha.kubernetes.io/ttl:0 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [] []},Spec:NodeSpec{PodCIDR:10.244.0.0/24,DoNotUseExternalID:,ProviderID:,Unschedulable:false,Taints:[]Taint{Taint{Key:node.kubernetes.io/not-ready,Value:,Effect:NoSchedule,TimeAdded:2025-07-31 08:02:44 +0000 UTC,},Taint{Key:node.kubernetes.io/unreachable,Value:,Effect:NoSchedule,TimeAdded:2025-07-31 08:20:42 +0000 UTC,},},ConfigSource:nil,PodCIDRs:[10.244.0.0/24],},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{118516367360 0} {<nil>}  BinarySI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8221097984 0} {<nil>}  BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{118516367360 0} {<nil>}  BinarySI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8221097984 0} {<nil>}  BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2025-07-31 08:20:50 +0000 UTC,LastTransitionTime:2025-07-31 08:20:50 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2025-07-31 08:20:51 +0000 UTC,LastTransitionTime:2025-07-31 08:20:51 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2025-07-31 08:20:51 +0000 UTC,LastTransitionTime:2025-07-31 08:20:51 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2025-07-31 08:20:51 +0000 UTC,LastTransitionTime:2025-07-31 08:20:51 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:192.168.49.2,},NodeAddress{Type:Hostname,Address:minikube,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:aba78c4910114253a61f9369b53df5f7,SystemUUID:aba78c4910114253a61f9369b53df5f7,BootID:db498821-09de-4cb3-b39d-286a131418d7,KernelVersion:6.10.14-linuxkit,OSImage:Ubuntu 22.04.5 LTS,ContainerRuntimeVersion:docker://28.1.1,KubeletVersion:v1.33.1,KubeProxyVersion:,OperatingSystem:linux,Architecture:amd64,Swap:&NodeSwapStatus{Capacity:*1073737728,},},Images:[]ContainerImage{ContainerImage{Names:[trainwithshubham/notes-app-k8s@sha256:1b1528e261028c74ecf0e5f4fc4fb078580b16a27bc28401ae3d74ba46bf1ea4 trainwithshubham/notes-app-k8s:latest],SizeBytes:1056395054,},ContainerImage{Names:[mongo@sha256:a6bda40d00e56682aeaa1bfc88e024b7dd755782c575c02760104fe02010f94f mongo:latest],SizeBytes:908485708,},ContainerImage{Names:[grafana/grafana@sha256:6ac590e7cabc2fbe8d7b8fc1ce9c9f0582177b334e0df9c927ebd9670469440f grafana/grafana:12.1.0],SizeBytes:722284119,},ContainerImage{Names:[registry.k8s.io/ingress-nginx/controller@sha256:dc75a7baec7a3b827a5d7ab0acd10ab507904c7dad692365b3e3b596eca1afd2],SizeBytes:317817911,},ContainerImage{Names:[quay.io/prometheus/prometheus@sha256:63805ebb8d2b3920190daf1cb14a60871b16fd38bed42b857a3182bc621f4996 quay.io/prometheus/prometheus:v3.5.0],SizeBytes:313023893,},ContainerImage{Names:[k8s.gcr.io/ingress-nginx/controller@sha256:7612338342a1e7b8090bef78f2a04fffcadd548ccaabe8a47bf7758ff549a5f7 registry.k8s.io/ingress-nginx/controller@sha256:7612338342a1e7b8090bef78f2a04fffcadd548ccaabe8a47bf7758ff549a5f7 k8s.gcr.io/ingress-nginx/controller:v1.7.0 registry.k8s.io/ingress-nginx/controller:v1.7.0],SizeBytes:282544440,},ContainerImage{Names:[registry.k8s.io/ingress-nginx/controller@sha256:e24f39d3eed6bcc239a56f20098878845f62baa34b9f2be2fd2c38ce9fb0f29e],SizeBytes:281753758,},ContainerImage{Names:[registry.k8s.io/ingress-nginx/controller@sha256:42b3f0e5d0846876b1791cd3afeb5f1cbbe4259d6f35651dcc1b5c980925379c],SizeBytes:273236489,},ContainerImage{Names:[registry.k8s.io/ingress-nginx/controller@sha256:54f7fe2c6c5a9db9a0ebf1131797109bb7a4d91f56b9b362bde2abd237dd1974],SizeBytes:262576970,},ContainerImage{Names:[registry.k8s.io/ingress-nginx/controller@sha256:b3aba22b1da80e7acfc52b115cae1d4c687172cbf2b742d5b502419c25ff340e],SizeBytes:253777407,},ContainerImage{Names:[grafana/promtail@sha256:b338a29de45ef8ffa96f882f3a36306b1e61262b2a560ff523e0e2633cccbbc4 grafana/promtail:2.9.3],SizeBytes:197598413,},ContainerImage{Names:[nginx@sha256:84ec966e61a8c7846f509da7eb081c55c1d56817448728924a87ab32f12a72fb nginx:latest],SizeBytes:192231837,},ContainerImage{Names:[registry.k8s.io/etcd@sha256:d58c035df557080a27387d687092e3fc2b64c6d0e3162dc51453a115f847d121 registry.k8s.io/etcd:3.5.21-0],SizeBytes:153121922,},ContainerImage{Names:[nikhil845/chatapp-backend@sha256:730bd1cf80be4a8209f94ebc3883f22512b127122542ee8e6ae60ba8f08b25e1 nikhil845/chatapp-backend:latest],SizeBytes:146717795,},ContainerImage{Names:[registry.k8s.io/kube-apiserver@sha256:d8ae2fb01c39aa1c7add84f3d54425cf081c24c11e3946830292a8cfa4293548 registry.k8s.io/kube-apiserver:v1.33.1],SizeBytes:101797831,},ContainerImage{Names:[registry.k8s.io/kube-proxy@sha256:7ddf379897139ae8ade8b33cb9373b70c632a4d5491da6e234f5d830e0a50807 registry.k8s.io/kube-proxy:v1.33.1],SizeBytes:97885628,},ContainerImage{Names:[registry.k8s.io/kube-controller-manager@sha256:7c9bea694e3a3c01ed6a5ee02d55a6124cc08e0b2eec6caa33f2c396b8cbc3f8 registry.k8s.io/kube-controller-manager:v1.33.1],SizeBytes:94597414,},ContainerImage{Names:[registry.k8s.io/kube-scheduler@sha256:395b7de7cdbdcc3c3a3db270844a3f71d757e2447a1e4db76b4cce46fba7fd55 registry.k8s.io/kube-scheduler:v1.33.1],SizeBytes:73441668,},ContainerImage{Names:[quay.io/prometheus/alertmanager@sha256:27c475db5fb156cab31d5c18a4251ac7ed567746a2483ff264516437a39b15ba quay.io/prometheus/alertmanager:v0.28.1],SizeBytes:72306219,},ContainerImage{Names:[registry.k8s.io/coredns/coredns@sha256:40384aa1f5ea6bfdc77997d243aec73da05f27aed0c5e9d65bfa98933c519d97 registry.k8s.io/coredns/coredns:v1.12.0],SizeBytes:70112656,},ContainerImage{Names:[registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:c9f76a75fd00e975416ea1b73300efd413116de0de8570346ed90766c5b5cefb registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.0],SizeBytes:69784602,},ContainerImage{Names:[quay.io/prometheus-operator/prometheus-operator@sha256:b6a89b8ec08f4cca759b2d579e8545f97ffb897973fcd68148c153f2e936c8b3 quay.io/prometheus-operator/prometheus-operator:v0.83.0],SizeBytes:68023841,},ContainerImage{Names:[grafana/loki@sha256:1ee60f980950b00e505bd564b40f720132a0653b110e993043bb5940673d060a grafana/loki:2.6.1],SizeBytes:61847961,},ContainerImage{Names:[registry.k8s.io/kube-state-metrics/kube-state-metrics@sha256:e750cd4b43f782e3106537026c2995cac85d921aedea334e1d16caad7877c360 registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.16.0],SizeBytes:60729925,},ContainerImage{Names:[registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:36d05b4077fb8e3d13663702fa337f124675ba8667cbd949c03a8e8ea6fa4366],SizeBytes:55313114,},ContainerImage{Names:[nikhil845/chatapp-frontend@sha256:c2c8dc7abe2b85c85720ce7b502f02674d4aa472459b02d854919bfd32cfa68d],SizeBytes:54877293,},ContainerImage{Names:[nikhil845/chatapp-frontend@sha256:ad50a2135c283b828760b6e1fd8491daf28bc2b5a32401a54e9d6a2b21f7dc4f nikhil845/chatapp-frontend:latest],SizeBytes:54877248,},ContainerImage{Names:[registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80],SizeBytes:53651320,},ContainerImage{Names:[quay.io/prometheus-operator/prometheus-config-reloader@sha256:697a5fdc409d8fbec55cfa389b62bbde04e74aaa6b67485a0ab8bd4ee5ab4b5c quay.io/prometheus-operator/prometheus-config-reloader:v0.84.0],SizeBytes:45831713,},ContainerImage{Names:[registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:549e71a6ca248c5abd51cdb73dbc3083df62cf92ed5e6147c780e30f7e007a47],SizeBytes:45702413,},ContainerImage{Names:[quay.io/prometheus-operator/prometheus-config-reloader@sha256:78aec597d87aa2b4ba947ab9190538dae93a58a67b8e930aefea1086534b02ef quay.io/prometheus-operator/prometheus-config-reloader:v0.83.0],SizeBytes:45516321,},ContainerImage{Names:[gcr.io/k8s-minikube/storage-provisioner@sha256:18eb69d1418e854ad5a19e399310e52808a8321e4c441c1dddad8977a0d7a944 gcr.io/k8s-minikube/storage-provisioner:v5],SizeBytes:31465472,},ContainerImage{Names:[gcr.io/google-samples/hello-app@sha256:6a077363c1681a48c5249eca2cdfd8f41706992c174733550a736ba9ec5600cd gcr.io/google-samples/hello-app:1.0],SizeBytes:28719251,},ContainerImage{Names:[quay.io/prometheus/node-exporter@sha256:d00a542e409ee618a4edc67da14dd48c5da66726bbd5537ab2af9c1dfc442c8a quay.io/prometheus/node-exporter:v1.9.1],SizeBytes:24978622,},ContainerImage{Names:[quay.io/prometheus/pushgateway@sha256:03738d278e082ee9821df730c741b3b465c251fc2b68a85883def301a55a6215 quay.io/prometheus/pushgateway:v1.11.1],SizeBytes:24169217,},ContainerImage{Names:[busybox@sha256:f85340bf132ae937d2c2a763b8335c9bab35d6e8293f70f606b9c6178d84f42b busybox:latest],SizeBytes:4277910,},ContainerImage{Names:[registry.k8s.io/pause@sha256:ee6521f290b2168b6e0935a181d4cff9be1ac3f505666ef0e3c98fae8199917a registry.k8s.io/pause:3.10],SizeBytes:735760,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,RuntimeHandlers:[]NodeRuntimeHandler{NodeRuntimeHandler{Name:,Features:&NodeRuntimeHandlerFeatures{RecursiveReadOnlyMounts:*true,UserNamespaces:*false,},},NodeRuntimeHandler{Name:io.containerd.runc.v2,Features:&NodeRuntimeHandlerFeatures{RecursiveReadOnlyMounts:*true,UserNamespaces:*false,},},NodeRuntimeHandler{Name:runc,Features:&NodeRuntimeHandlerFeatures{RecursiveReadOnlyMounts:*true,UserNamespaces:*false,},},},Features:nil,},}" logger="node-lifecycle-controller" node="minikube"
E0731 08:32:09.897865       1 controller_utils.go:215] "Unhandled Error" err="unable to remove [&Taint{Key:node.kubernetes.io/unreachable,Value:,Effect:NoSchedule,TimeAdded:2025-07-31 08:20:42 +0000 UTC,}] unneeded taint from unresponsive Node \"minikube\": Get \"https://192.168.49.2:8443/api/v1/nodes/minikube?resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
E0731 08:32:09.934025       1 stateful_set.go:438] "Unhandled Error" err="error syncing StatefulSet default/loki, requeuing: Put \"https://192.168.49.2:8443/apis/apps/v1/namespaces/default/statefulsets/loki/status\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
E0731 08:32:09.976809       1 node_lifecycle_controller.go:514] "Failed to taint NoSchedule on node, requeue it" err="failed to swap taints of node &Node{ObjectMeta:{minikube    83d775f7-10dc-437d-8d32-ae49037a8a55 66675 0 2025-07-16 09:18:26 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/os:linux kubernetes.io/arch:amd64 kubernetes.io/hostname:minikube kubernetes.io/os:linux minikube.k8s.io/commit:f8f52f5de11fc6ad8244afac475e1d0f96841df1-dirty minikube.k8s.io/name:minikube minikube.k8s.io/primary:true minikube.k8s.io/updated_at:2025_07_16T14_48_30_0700 minikube.k8s.io/version:v1.36.0 node-role.kubernetes.io/control-plane: node.kubernetes.io/exclude-from-external-load-balancers:] map[kubeadm.alpha.kubernetes.io/cri-socket:unix:///var/run/cri-dockerd.sock node.alpha.kubernetes.io/ttl:0 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [] []},Spec:NodeSpec{PodCIDR:10.244.0.0/24,DoNotUseExternalID:,ProviderID:,Unschedulable:false,Taints:[]Taint{Taint{Key:node.kubernetes.io/unreachable,Value:,Effect:NoSchedule,TimeAdded:2025-07-31 08:20:42 +0000 UTC,},},ConfigSource:nil,PodCIDRs:[10.244.0.0/24],},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{118516367360 0} {<nil>}  BinarySI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8221097984 0} {<nil>}  BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{118516367360 0} {<nil>}  BinarySI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8221097984 0} {<nil>}  BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2025-07-31 08:20:50 +0000 UTC,LastTransitionTime:2025-07-31 08:20:50 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2025-07-31 08:20:51 +0000 UTC,LastTransitionTime:2025-07-31 08:20:51 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2025-07-31 08:20:51 +0000 UTC,LastTransitionTime:2025-07-31 08:20:51 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2025-07-31 08:20:51 +0000 UTC,LastTransitionTime:2025-07-31 08:20:51 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:192.168.49.2,},NodeAddress{Type:Hostname,Address:minikube,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:aba78c4910114253a61f9369b53df5f7,SystemUUID:aba78c4910114253a61f9369b53df5f7,BootID:db498821-09de-4cb3-b39d-286a131418d7,KernelVersion:6.10.14-linuxkit,OSImage:Ubuntu 22.04.5 LTS,ContainerRuntimeVersion:docker://28.1.1,KubeletVersion:v1.33.1,KubeProxyVersion:,OperatingSystem:linux,Architecture:amd64,Swap:&NodeSwapStatus{Capacity:*1073737728,},},Images:[]ContainerImage{ContainerImage{Names:[trainwithshubham/notes-app-k8s@sha256:1b1528e261028c74ecf0e5f4fc4fb078580b16a27bc28401ae3d74ba46bf1ea4 trainwithshubham/notes-app-k8s:latest],SizeBytes:1056395054,},ContainerImage{Names:[mongo@sha256:a6bda40d00e56682aeaa1bfc88e024b7dd755782c575c02760104fe02010f94f mongo:latest],SizeBytes:908485708,},ContainerImage{Names:[grafana/grafana@sha256:6ac590e7cabc2fbe8d7b8fc1ce9c9f0582177b334e0df9c927ebd9670469440f grafana/grafana:12.1.0],SizeBytes:722284119,},ContainerImage{Names:[registry.k8s.io/ingress-nginx/controller@sha256:dc75a7baec7a3b827a5d7ab0acd10ab507904c7dad692365b3e3b596eca1afd2],SizeBytes:317817911,},ContainerImage{Names:[quay.io/prometheus/prometheus@sha256:63805ebb8d2b3920190daf1cb14a60871b16fd38bed42b857a3182bc621f4996 quay.io/prometheus/prometheus:v3.5.0],SizeBytes:313023893,},ContainerImage{Names:[k8s.gcr.io/ingress-nginx/controller@sha256:7612338342a1e7b8090bef78f2a04fffcadd548ccaabe8a47bf7758ff549a5f7 registry.k8s.io/ingress-nginx/controller@sha256:7612338342a1e7b8090bef78f2a04fffcadd548ccaabe8a47bf7758ff549a5f7 k8s.gcr.io/ingress-nginx/controller:v1.7.0 registry.k8s.io/ingress-nginx/controller:v1.7.0],SizeBytes:282544440,},ContainerImage{Names:[registry.k8s.io/ingress-nginx/controller@sha256:e24f39d3eed6bcc239a56f20098878845f62baa34b9f2be2fd2c38ce9fb0f29e],SizeBytes:281753758,},ContainerImage{Names:[registry.k8s.io/ingress-nginx/controller@sha256:42b3f0e5d0846876b1791cd3afeb5f1cbbe4259d6f35651dcc1b5c980925379c],SizeBytes:273236489,},ContainerImage{Names:[registry.k8s.io/ingress-nginx/controller@sha256:54f7fe2c6c5a9db9a0ebf1131797109bb7a4d91f56b9b362bde2abd237dd1974],SizeBytes:262576970,},ContainerImage{Names:[registry.k8s.io/ingress-nginx/controller@sha256:b3aba22b1da80e7acfc52b115cae1d4c687172cbf2b742d5b502419c25ff340e],SizeBytes:253777407,},ContainerImage{Names:[grafana/promtail@sha256:b338a29de45ef8ffa96f882f3a36306b1e61262b2a560ff523e0e2633cccbbc4 grafana/promtail:2.9.3],SizeBytes:197598413,},ContainerImage{Names:[nginx@sha256:84ec966e61a8c7846f509da7eb081c55c1d56817448728924a87ab32f12a72fb nginx:latest],SizeBytes:192231837,},ContainerImage{Names:[registry.k8s.io/etcd@sha256:d58c035df557080a27387d687092e3fc2b64c6d0e3162dc51453a115f847d121 registry.k8s.io/etcd:3.5.21-0],SizeBytes:153121922,},ContainerImage{Names:[nikhil845/chatapp-backend@sha256:730bd1cf80be4a8209f94ebc3883f22512b127122542ee8e6ae60ba8f08b25e1 nikhil845/chatapp-backend:latest],SizeBytes:146717795,},ContainerImage{Names:[registry.k8s.io/kube-apiserver@sha256:d8ae2fb01c39aa1c7add84f3d54425cf081c24c11e3946830292a8cfa4293548 registry.k8s.io/kube-apiserver:v1.33.1],SizeBytes:101797831,},ContainerImage{Names:[registry.k8s.io/kube-proxy@sha256:7ddf379897139ae8ade8b33cb9373b70c632a4d5491da6e234f5d830e0a50807 registry.k8s.io/kube-proxy:v1.33.1],SizeBytes:97885628,},ContainerImage{Names:[registry.k8s.io/kube-controller-manager@sha256:7c9bea694e3a3c01ed6a5ee02d55a6124cc08e0b2eec6caa33f2c396b8cbc3f8 registry.k8s.io/kube-controller-manager:v1.33.1],SizeBytes:94597414,},ContainerImage{Names:[registry.k8s.io/kube-scheduler@sha256:395b7de7cdbdcc3c3a3db270844a3f71d757e2447a1e4db76b4cce46fba7fd55 registry.k8s.io/kube-scheduler:v1.33.1],SizeBytes:73441668,},ContainerImage{Names:[quay.io/prometheus/alertmanager@sha256:27c475db5fb156cab31d5c18a4251ac7ed567746a2483ff264516437a39b15ba quay.io/prometheus/alertmanager:v0.28.1],SizeBytes:72306219,},ContainerImage{Names:[registry.k8s.io/coredns/coredns@sha256:40384aa1f5ea6bfdc77997d243aec73da05f27aed0c5e9d65bfa98933c519d97 registry.k8s.io/coredns/coredns:v1.12.0],SizeBytes:70112656,},ContainerImage{Names:[registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:c9f76a75fd00e975416ea1b73300efd413116de0de8570346ed90766c5b5cefb registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.0],SizeBytes:69784602,},ContainerImage{Names:[quay.io/prometheus-operator/prometheus-operator@sha256:b6a89b8ec08f4cca759b2d579e8545f97ffb897973fcd68148c153f2e936c8b3 quay.io/prometheus-operator/prometheus-operator:v0.83.0],SizeBytes:68023841,},ContainerImage{Names:[grafana/loki@sha256:1ee60f980950b00e505bd564b40f720132a0653b110e993043bb5940673d060a grafana/loki:2.6.1],SizeBytes:61847961,},ContainerImage{Names:[registry.k8s.io/kube-state-metrics/kube-state-metrics@sha256:e750cd4b43f782e3106537026c2995cac85d921aedea334e1d16caad7877c360 registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.16.0],SizeBytes:60729925,},ContainerImage{Names:[registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:36d05b4077fb8e3d13663702fa337f124675ba8667cbd949c03a8e8ea6fa4366],SizeBytes:55313114,},ContainerImage{Names:[nikhil845/chatapp-frontend@sha256:c2c8dc7abe2b85c85720ce7b502f02674d4aa472459b02d854919bfd32cfa68d],SizeBytes:54877293,},ContainerImage{Names:[nikhil845/chatapp-frontend@sha256:ad50a2135c283b828760b6e1fd8491daf28bc2b5a32401a54e9d6a2b21f7dc4f nikhil845/chatapp-frontend:latest],SizeBytes:54877248,},ContainerImage{Names:[registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80],SizeBytes:53651320,},ContainerImage{Names:[quay.io/prometheus-operator/prometheus-config-reloader@sha256:697a5fdc409d8fbec55cfa389b62bbde04e74aaa6b67485a0ab8bd4ee5ab4b5c quay.io/prometheus-operator/prometheus-config-reloader:v0.84.0],SizeBytes:45831713,},ContainerImage{Names:[registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:549e71a6ca248c5abd51cdb73dbc3083df62cf92ed5e6147c780e30f7e007a47],SizeBytes:45702413,},ContainerImage{Names:[quay.io/prometheus-operator/prometheus-config-reloader@sha256:78aec597d87aa2b4ba947ab9190538dae93a58a67b8e930aefea1086534b02ef quay.io/prometheus-operator/prometheus-config-reloader:v0.83.0],SizeBytes:45516321,},ContainerImage{Names:[gcr.io/k8s-minikube/storage-provisioner@sha256:18eb69d1418e854ad5a19e399310e52808a8321e4c441c1dddad8977a0d7a944 gcr.io/k8s-minikube/storage-provisioner:v5],SizeBytes:31465472,},ContainerImage{Names:[gcr.io/google-samples/hello-app@sha256:6a077363c1681a48c5249eca2cdfd8f41706992c174733550a736ba9ec5600cd gcr.io/google-samples/hello-app:1.0],SizeBytes:28719251,},ContainerImage{Names:[quay.io/prometheus/node-exporter@sha256:d00a542e409ee618a4edc67da14dd48c5da66726bbd5537ab2af9c1dfc442c8a quay.io/prometheus/node-exporter:v1.9.1],SizeBytes:24978622,},ContainerImage{Names:[quay.io/prometheus/pushgateway@sha256:03738d278e082ee9821df730c741b3b465c251fc2b68a85883def301a55a6215 quay.io/prometheus/pushgateway:v1.11.1],SizeBytes:24169217,},ContainerImage{Names:[busybox@sha256:f85340bf132ae937d2c2a763b8335c9bab35d6e8293f70f606b9c6178d84f42b busybox:latest],SizeBytes:4277910,},ContainerImage{Names:[registry.k8s.io/pause@sha256:ee6521f290b2168b6e0935a181d4cff9be1ac3f505666ef0e3c98fae8199917a registry.k8s.io/pause:3.10],SizeBytes:735760,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,RuntimeHandlers:[]NodeRuntimeHandler{NodeRuntimeHandler{Name:,Features:&NodeRuntimeHandlerFeatures{RecursiveReadOnlyMounts:*true,UserNamespaces:*false,},},NodeRuntimeHandler{Name:io.containerd.runc.v2,Features:&NodeRuntimeHandlerFeatures{RecursiveReadOnlyMounts:*true,UserNamespaces:*false,},},NodeRuntimeHandler{Name:runc,Features:&NodeRuntimeHandlerFeatures{RecursiveReadOnlyMounts:*true,UserNamespaces:*false,},},},Features:nil,},}" logger="node-lifecycle-controller" node="minikube"
E0731 08:32:10.144090       1 stateful_set.go:438] "Unhandled Error" err="error syncing StatefulSet default/loki, requeuing: Put \"https://192.168.49.2:8443/apis/apps/v1/namespaces/default/statefulsets/loki/status\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
I0731 08:32:10.153687       1 endpointslice_controller.go:344] "Error syncing endpoint slices for service, retrying" logger="endpointslice-controller" key="default/loki" err="failed to update loki-hfdpr EndpointSlice for Service default/loki: Put \"https://192.168.49.2:8443/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/loki-hfdpr\": dial tcp 192.168.49.2:8443: connect: connection refused"
I0731 08:32:10.153850       1 event.go:377] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"loki", UID:"b93db4e7-767f-43f6-a0a2-75aa1914dc98", APIVersion:"v1", ResourceVersion:"49821", FieldPath:""}): type: 'Warning' reason: 'FailedToUpdateEndpointSlices' Error updating Endpoint Slices for Service default/loki: failed to update loki-hfdpr EndpointSlice for Service default/loki: Put "https://192.168.49.2:8443/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/loki-hfdpr": dial tcp 192.168.49.2:8443: connect: connection refused
I0731 08:32:10.243746       1 endpointslice_controller.go:344] "Error syncing endpoint slices for service, retrying" logger="endpointslice-controller" key="default/loki-memberlist" err="failed to update loki-memberlist-lzsjt EndpointSlice for Service default/loki-memberlist: Put \"https://192.168.49.2:8443/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/loki-memberlist-lzsjt\": dial tcp 192.168.49.2:8443: connect: connection refused"
I0731 08:32:10.243807       1 event.go:377] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"loki-memberlist", UID:"c1320576-de64-486f-ae3b-f926cc65b36a", APIVersion:"v1", ResourceVersion:"49814", FieldPath:""}): type: 'Warning' reason: 'FailedToUpdateEndpointSlices' Error updating Endpoint Slices for Service default/loki-memberlist: failed to update loki-memberlist-lzsjt EndpointSlice for Service default/loki-memberlist: Put "https://192.168.49.2:8443/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/loki-memberlist-lzsjt": dial tcp 192.168.49.2:8443: connect: connection refused
I0731 08:32:10.245529       1 endpointslice_controller.go:344] "Error syncing endpoint slices for service, retrying" logger="endpointslice-controller" key="default/loki-headless" err="failed to update loki-headless-fhrrh EndpointSlice for Service default/loki-headless: Put \"https://192.168.49.2:8443/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/loki-headless-fhrrh\": dial tcp 192.168.49.2:8443: connect: connection refused"
I0731 08:32:10.245577       1 event.go:377] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"loki-headless", UID:"1daa4723-0bbe-45bd-87df-97946d1a5c14", APIVersion:"v1", ResourceVersion:"49813", FieldPath:""}): type: 'Warning' reason: 'FailedToUpdateEndpointSlices' Error updating Endpoint Slices for Service default/loki-headless: failed to update loki-headless-fhrrh EndpointSlice for Service default/loki-headless: Put "https://192.168.49.2:8443/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/loki-headless-fhrrh": dial tcp 192.168.49.2:8443: connect: connection refused
E0731 08:32:10.353560       1 stateful_set.go:438] "Unhandled Error" err="error syncing StatefulSet default/loki, requeuing: Put \"https://192.168.49.2:8443/apis/apps/v1/namespaces/default/statefulsets/loki/status\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
E0731 08:32:10.684658       1 stateful_set.go:438] "Unhandled Error" err="error syncing StatefulSet default/loki, requeuing: Put \"https://192.168.49.2:8443/apis/apps/v1/namespaces/default/statefulsets/loki/status\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
E0731 08:32:11.333655       1 stateful_set.go:438] "Unhandled Error" err="error syncing StatefulSet default/loki, requeuing: Put \"https://192.168.49.2:8443/apis/apps/v1/namespaces/default/statefulsets/loki/status\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
I0731 08:32:12.155897       1 endpointslice_controller.go:344] "Error syncing endpoint slices for service, retrying" logger="endpointslice-controller" key="default/loki" err="failed to update loki-hfdpr EndpointSlice for Service default/loki: Put \"https://192.168.49.2:8443/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/loki-hfdpr\": dial tcp 192.168.49.2:8443: connect: connection refused"
I0731 08:32:12.155943       1 event.go:377] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"loki", UID:"b93db4e7-767f-43f6-a0a2-75aa1914dc98", APIVersion:"v1", ResourceVersion:"49821", FieldPath:""}): type: 'Warning' reason: 'FailedToUpdateEndpointSlices' Error updating Endpoint Slices for Service default/loki: failed to update loki-hfdpr EndpointSlice for Service default/loki: Put "https://192.168.49.2:8443/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/loki-hfdpr": dial tcp 192.168.49.2:8443: connect: connection refused
I0731 08:32:12.250687       1 endpointslice_controller.go:344] "Error syncing endpoint slices for service, retrying" logger="endpointslice-controller" key="default/loki-memberlist" err="failed to update loki-memberlist-lzsjt EndpointSlice for Service default/loki-memberlist: Put \"https://192.168.49.2:8443/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/loki-memberlist-lzsjt\": dial tcp 192.168.49.2:8443: connect: connection refused"
I0731 08:32:12.250734       1 event.go:377] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"loki-memberlist", UID:"c1320576-de64-486f-ae3b-f926cc65b36a", APIVersion:"v1", ResourceVersion:"49814", FieldPath:""}): type: 'Warning' reason: 'FailedToUpdateEndpointSlices' Error updating Endpoint Slices for Service default/loki-memberlist: failed to update loki-memberlist-lzsjt EndpointSlice for Service default/loki-memberlist: Put "https://192.168.49.2:8443/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/loki-memberlist-lzsjt": dial tcp 192.168.49.2:8443: connect: connection refused
I0731 08:32:12.253624       1 endpointslice_controller.go:344] "Error syncing endpoint slices for service, retrying" logger="endpointslice-controller" key="default/loki-headless" err="failed to update loki-headless-fhrrh EndpointSlice for Service default/loki-headless: Put \"https://192.168.49.2:8443/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/loki-headless-fhrrh\": dial tcp 192.168.49.2:8443: connect: connection refused"
I0731 08:32:12.253661       1 event.go:377] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"loki-headless", UID:"1daa4723-0bbe-45bd-87df-97946d1a5c14", APIVersion:"v1", ResourceVersion:"49813", FieldPath:""}): type: 'Warning' reason: 'FailedToUpdateEndpointSlices' Error updating Endpoint Slices for Service default/loki-headless: failed to update loki-headless-fhrrh EndpointSlice for Service default/loki-headless: Put "https://192.168.49.2:8443/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/loki-headless-fhrrh": dial tcp 192.168.49.2:8443: connect: connection refused
E0731 08:32:12.636509       1 stateful_set.go:438] "Unhandled Error" err="error syncing StatefulSet default/loki, requeuing: Put \"https://192.168.49.2:8443/apis/apps/v1/namespaces/default/statefulsets/loki/status\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
E0731 08:32:13.186002       1 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://192.168.49.2:8443/api/v1/namespaces/default/events\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="endpoints-controller" event="&Event{ObjectMeta:{loki.185748e52442fdf0  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Endpoints,Namespace:default,Name:loki,UID:9f6b9bd2-cdf1-4cd7-8243-ba9717061d4a,APIVersion:v1,ResourceVersion:66661,FieldPath:,},Reason:FailedToUpdateEndpoint,Message:Failed to update endpoint default/loki: Put \"https://192.168.49.2:8443/api/v1/namespaces/default/endpoints/loki\": dial tcp 192.168.49.2:8443: connect: connection refused,Source:EventSource{Component:endpoint-controller,Host:,},FirstTimestamp:2025-07-31 08:32:08.877178352 +0000 UTC m=+7059.354567437,LastTimestamp:2025-07-31 08:32:08.877178352 +0000 UTC m=+7059.354567437,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:endpoint-controller,ReportingInstance:,}"
E0731 08:32:14.422578       1 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://192.168.49.2:8443/api/v1/namespaces/default/events\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="endpointslice-controller" event="&Event{ObjectMeta:{loki.185748e52b4c6f08  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Service,Namespace:default,Name:loki,UID:b93db4e7-767f-43f6-a0a2-75aa1914dc98,APIVersion:v1,ResourceVersion:49821,FieldPath:,},Reason:FailedToUpdateEndpointSlices,Message:Error updating Endpoint Slices for Service default/loki: failed to update loki-hfdpr EndpointSlice for Service default/loki: Put \"https://192.168.49.2:8443/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/loki-hfdpr\": dial tcp 192.168.49.2:8443: connect: connection refused,Source:EventSource{Component:endpoint-slice-controller,Host:,},FirstTimestamp:2025-07-31 08:32:08.99523764 +0000 UTC m=+7059.472626716,LastTimestamp:2025-07-31 08:32:08.99523764 +0000 UTC m=+7059.472626716,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:endpoint-slice-controller,ReportingInstance:,}"
E0731 08:32:15.228772       1 stateful_set.go:438] "Unhandled Error" err="error syncing StatefulSet default/loki, requeuing: Put \"https://192.168.49.2:8443/apis/apps/v1/namespaces/default/statefulsets/loki/status\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
I0731 08:32:16.176054       1 endpointslice_controller.go:344] "Error syncing endpoint slices for service, retrying" logger="endpointslice-controller" key="default/loki" err="failed to update loki-hfdpr EndpointSlice for Service default/loki: Put \"https://192.168.49.2:8443/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/loki-hfdpr\": dial tcp 192.168.49.2:8443: connect: connection refused"
I0731 08:32:16.181665       1 event.go:377] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"loki", UID:"b93db4e7-767f-43f6-a0a2-75aa1914dc98", APIVersion:"v1", ResourceVersion:"49821", FieldPath:""}): type: 'Warning' reason: 'FailedToUpdateEndpointSlices' Error updating Endpoint Slices for Service default/loki: failed to update loki-hfdpr EndpointSlice for Service default/loki: Put "https://192.168.49.2:8443/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/loki-hfdpr": dial tcp 192.168.49.2:8443: connect: connection refused
I0731 08:32:16.257738       1 endpointslice_controller.go:344] "Error syncing endpoint slices for service, retrying" logger="endpointslice-controller" key="default/loki-memberlist" err="failed to update loki-memberlist-lzsjt EndpointSlice for Service default/loki-memberlist: Put \"https://192.168.49.2:8443/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/loki-memberlist-lzsjt\": dial tcp 192.168.49.2:8443: connect: connection refused"
I0731 08:32:16.256801       1 endpointslice_controller.go:344] "Error syncing endpoint slices for service, retrying" logger="endpointslice-controller" key="default/loki-headless" err="failed to update loki-headless-fhrrh EndpointSlice for Service default/loki-headless: Put \"https://192.168.49.2:8443/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/loki-headless-fhrrh\": dial tcp 192.168.49.2:8443: connect: connection refused"
I0731 08:32:16.258306       1 event.go:377] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"loki-headless", UID:"1daa4723-0bbe-45bd-87df-97946d1a5c14", APIVersion:"v1", ResourceVersion:"49813", FieldPath:""}): type: 'Warning' reason: 'FailedToUpdateEndpointSlices' Error updating Endpoint Slices for Service default/loki-headless: failed to update loki-headless-fhrrh EndpointSlice for Service default/loki-headless: Put "https://192.168.49.2:8443/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/loki-headless-fhrrh": dial tcp 192.168.49.2:8443: connect: connection refused
I0731 08:32:16.258325       1 event.go:377] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"loki-memberlist", UID:"c1320576-de64-486f-ae3b-f926cc65b36a", APIVersion:"v1", ResourceVersion:"49814", FieldPath:""}): type: 'Warning' reason: 'FailedToUpdateEndpointSlices' Error updating Endpoint Slices for Service default/loki-memberlist: failed to update loki-memberlist-lzsjt EndpointSlice for Service default/loki-memberlist: Put "https://192.168.49.2:8443/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/loki-memberlist-lzsjt": dial tcp 192.168.49.2:8443: connect: connection refused
E0731 08:32:18.550081       1 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://192.168.49.2:8443/api/v1/namespaces/default/events\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="taint-eviction-controller" event="&Event{ObjectMeta:{prometheus-kube-prometheus-operator-d89fb8945-f7n5z.1857484a79571fc4  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Pod,Namespace:default,Name:prometheus-kube-prometheus-operator-d89fb8945-f7n5z,UID:,APIVersion:v1,ResourceVersion:,FieldPath:,},Reason:TaintManagerEviction,Message:Cancelling deletion of Pod default/prometheus-kube-prometheus-operator-d89fb8945-f7n5z,Source:EventSource{Component:taint-eviction-controller,Host:,},FirstTimestamp:2025-07-31 08:21:04.584630212 +0000 UTC m=+7053.646674505,LastTimestamp:2025-07-31 08:21:04.584630212 +0000 UTC m=+7053.646674505,Count:1,Type:Normal,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:taint-eviction-controller,ReportingInstance:,}"
I0731 08:32:19.460497       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="Get \"https://192.168.49.2:8443/api\": dial tcp 192.168.49.2:8443: connect: connection refused"
E0731 08:32:20.035903       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: Get \"https://192.168.49.2:8443/api\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
E0731 08:32:20.371844       1 stateful_set.go:438] "Unhandled Error" err="error syncing StatefulSet default/loki, requeuing: Put \"https://192.168.49.2:8443/apis/apps/v1/namespaces/default/statefulsets/loki/status\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
I0731 08:32:31.723644       1 endpointslice_controller.go:344] "Error syncing endpoint slices for service, retrying" logger="endpointslice-controller" key="default/loki-memberlist" err="failed to update loki-memberlist-lzsjt EndpointSlice for Service default/loki-memberlist: Unauthorized"
I0731 08:32:31.723857       1 endpointslice_controller.go:344] "Error syncing endpoint slices for service, retrying" logger="endpointslice-controller" key="default/loki" err="failed to update loki-hfdpr EndpointSlice for Service default/loki: Unauthorized"
I0731 08:32:31.723900       1 endpointslice_controller.go:344] "Error syncing endpoint slices for service, retrying" logger="endpointslice-controller" key="default/loki-headless" err="failed to update loki-headless-fhrrh EndpointSlice for Service default/loki-headless: Unauthorized"
E0731 08:32:31.727883       1 stateful_set.go:438] "Unhandled Error" err="error syncing StatefulSet default/loki, requeuing: Unauthorized" logger="UnhandledError"
I0731 08:32:31.733538       1 event.go:377] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"loki-memberlist", UID:"c1320576-de64-486f-ae3b-f926cc65b36a", APIVersion:"v1", ResourceVersion:"49814", FieldPath:""}): type: 'Warning' reason: 'FailedToUpdateEndpointSlices' Error updating Endpoint Slices for Service default/loki-memberlist: failed to update loki-memberlist-lzsjt EndpointSlice for Service default/loki-memberlist: Unauthorized
I0731 08:32:31.733918       1 event.go:377] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"loki", UID:"b93db4e7-767f-43f6-a0a2-75aa1914dc98", APIVersion:"v1", ResourceVersion:"49821", FieldPath:""}): type: 'Warning' reason: 'FailedToUpdateEndpointSlices' Error updating Endpoint Slices for Service default/loki: failed to update loki-hfdpr EndpointSlice for Service default/loki: Unauthorized
I0731 08:32:31.733963       1 event.go:377] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"loki-headless", UID:"1daa4723-0bbe-45bd-87df-97946d1a5c14", APIVersion:"v1", ResourceVersion:"49813", FieldPath:""}): type: 'Warning' reason: 'FailedToUpdateEndpointSlices' Error updating Endpoint Slices for Service default/loki-headless: failed to update loki-headless-fhrrh EndpointSlice for Service default/loki-headless: Unauthorized
E0731 08:32:31.775364       1 event.go:359] "Server rejected event (will not retry!)" err="Unauthorized" logger="endpointslice-controller" event="&Event{ObjectMeta:{loki.185748e52b4c6f08  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Service,Namespace:default,Name:loki,UID:b93db4e7-767f-43f6-a0a2-75aa1914dc98,APIVersion:v1,ResourceVersion:49821,FieldPath:,},Reason:FailedToUpdateEndpointSlices,Message:Error updating Endpoint Slices for Service default/loki: failed to update loki-hfdpr EndpointSlice for Service default/loki: Put \"https://192.168.49.2:8443/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/loki-hfdpr\": dial tcp 192.168.49.2:8443: connect: connection refused,Source:EventSource{Component:endpoint-slice-controller,Host:,},FirstTimestamp:2025-07-31 08:32:08.99523764 +0000 UTC m=+7059.472626716,LastTimestamp:2025-07-31 08:32:08.99523764 +0000 UTC m=+7059.472626716,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:endpoint-slice-controller,ReportingInstance:,}"
E0731 08:32:31.761076       1 event.go:359] "Server rejected event (will not retry!)" err="Unauthorized" logger="endpoints-controller" event="&Event{ObjectMeta:{loki.185748e52442fdf0  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Endpoints,Namespace:default,Name:loki,UID:9f6b9bd2-cdf1-4cd7-8243-ba9717061d4a,APIVersion:v1,ResourceVersion:66661,FieldPath:,},Reason:FailedToUpdateEndpoint,Message:Failed to update endpoint default/loki: Put \"https://192.168.49.2:8443/api/v1/namespaces/default/endpoints/loki\": dial tcp 192.168.49.2:8443: connect: connection refused,Source:EventSource{Component:endpoint-controller,Host:,},FirstTimestamp:2025-07-31 08:32:08.877178352 +0000 UTC m=+7059.354567437,LastTimestamp:2025-07-31 08:32:08.877178352 +0000 UTC m=+7059.354567437,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:endpoint-controller,ReportingInstance:,}"
E0731 08:32:31.807138       1 reflector.go:200] "Failed to watch" err="ipaddresses.networking.k8s.io is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"ipaddresses\" in API group \"networking.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.IPAddress"
E0731 08:32:31.807608       1 reflector.go:200] "Failed to watch" err="networkpolicies.networking.k8s.io is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"networkpolicies\" in API group \"networking.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.NetworkPolicy"
E0731 08:32:31.807638       1 reflector.go:200] "Failed to watch" err="replicasets.apps is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E0731 08:32:31.807660       1 reflector.go:200] "Failed to watch" err="endpointslices.discovery.k8s.io is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"endpointslices\" in API group \"discovery.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.EndpointSlice"
E0731 08:32:31.807682       1 reflector.go:200] "Failed to watch" err="poddisruptionbudgets.policy is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E0731 08:32:31.807711       1 reflector.go:200] "Failed to watch" err="clusterrolebindings.rbac.authorization.k8s.io is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"clusterrolebindings\" in API group \"rbac.authorization.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ClusterRoleBinding"
E0731 08:32:31.807739       1 reflector.go:200] "Failed to watch" err="priorityclasses.scheduling.k8s.io is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"priorityclasses\" in API group \"scheduling.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PriorityClass"
E0731 08:32:31.809035       1 reflector.go:200] "Failed to watch" err="podtemplates is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"podtemplates\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodTemplate"
E0731 08:32:31.809112       1 reflector.go:200] "Failed to watch" err="validatingadmissionpolicybindings.admissionregistration.k8s.io is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"validatingadmissionpolicybindings\" in API group \"admissionregistration.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ValidatingAdmissionPolicyBinding"
E0731 08:32:31.809187       1 reflector.go:200] "Failed to watch" err="persistentvolumeclaims is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E0731 08:32:31.809244       1 reflector.go:200] "Failed to watch" err="flowschemas.flowcontrol.apiserver.k8s.io is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"flowschemas\" in API group \"flowcontrol.apiserver.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.FlowSchema"
E0731 08:32:31.809259       1 reflector.go:200] "Failed to watch" err="apiservices.apiregistration.k8s.io is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"apiservices\" in API group \"apiregistration.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/metadata/metadatainformer/informer.go:138" type="*v1.PartialObjectMetadata"
E0731 08:32:31.810361       1 reflector.go:200] "Failed to watch" err="controllerrevisions.apps is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"controllerrevisions\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ControllerRevision"
E0731 08:32:31.810430       1 reflector.go:200] "Failed to watch" err="daemonsets.apps is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"daemonsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.DaemonSet"
E0731 08:32:31.812379       1 reflector.go:200] "Failed to watch" err="csidrivers.storage.k8s.io is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E0731 08:32:31.810881       1 reflector.go:200] "Failed to watch" err="probes.monitoring.coreos.com is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"probes\" in API group \"monitoring.coreos.com\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/metadata/metadatainformer/informer.go:138" type="*v1.PartialObjectMetadata"
E0731 08:32:31.810906       1 reflector.go:200] "Failed to watch" err="rolebindings.rbac.authorization.k8s.io is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"rolebindings\" in API group \"rbac.authorization.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.RoleBinding"
E0731 08:32:31.813010       1 reflector.go:200] "Failed to watch" err="prometheusagents.monitoring.coreos.com is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"prometheusagents\" in API group \"monitoring.coreos.com\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/metadata/metadatainformer/informer.go:138" type="*v1.PartialObjectMetadata"
E0731 08:32:31.814264       1 reflector.go:200] "Failed to watch" err="limitranges is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"limitranges\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.LimitRange"
E0731 08:32:31.814303       1 reflector.go:200] "Failed to watch" err="cronjobs.batch is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"cronjobs\" in API group \"batch\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CronJob"
E0731 08:32:31.814921       1 reflector.go:200] "Failed to watch" err="validatingadmissionpolicies.admissionregistration.k8s.io is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"validatingadmissionpolicies\" in API group \"admissionregistration.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ValidatingAdmissionPolicy"
E0731 08:32:31.814950       1 reflector.go:200] "Failed to watch" err="serviceaccounts is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"serviceaccounts\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ServiceAccount"
E0731 08:32:31.814969       1 reflector.go:200] "Failed to watch" err="csinodes.storage.k8s.io is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E0731 08:32:31.807127       1 reflector.go:200] "Failed to watch" err="prioritylevelconfigurations.flowcontrol.apiserver.k8s.io is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"prioritylevelconfigurations\" in API group \"flowcontrol.apiserver.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PriorityLevelConfiguration"
E0731 08:32:31.807054       1 reflector.go:200] "Failed to watch" err="alertmanagerconfigs.monitoring.coreos.com is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"alertmanagerconfigs\" in API group \"monitoring.coreos.com\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/metadata/metadatainformer/informer.go:138" type="*v1.PartialObjectMetadata"
E0731 08:32:31.815413       1 reflector.go:200] "Failed to watch" err="deployments.apps is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"deployments\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Deployment"
E0731 08:32:31.815507       1 reflector.go:200] "Failed to watch" err="runtimeclasses.node.k8s.io is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"runtimeclasses\" in API group \"node.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.RuntimeClass"
E0731 08:32:31.815527       1 reflector.go:200] "Failed to watch" err="storageclasses.storage.k8s.io is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E0731 08:32:31.815524       1 reflector.go:200] "Failed to watch" err="mutatingwebhookconfigurations.admissionregistration.k8s.io is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"mutatingwebhookconfigurations\" in API group \"admissionregistration.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.MutatingWebhookConfiguration"
E0731 08:32:31.815550       1 reflector.go:200] "Failed to watch" err="validatingwebhookconfigurations.admissionregistration.k8s.io is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"validatingwebhookconfigurations\" in API group \"admissionregistration.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ValidatingWebhookConfiguration"
E0731 08:32:31.815557       1 reflector.go:200] "Failed to watch" err="nodes is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0731 08:32:31.815568       1 reflector.go:200] "Failed to watch" err="prometheusrules.monitoring.coreos.com is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"prometheusrules\" in API group \"monitoring.coreos.com\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/metadata/metadatainformer/informer.go:138" type="*v1.PartialObjectMetadata"
E0731 08:32:31.815581       1 reflector.go:200] "Failed to watch" err="csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E0731 08:32:31.815584       1 reflector.go:200] "Failed to watch" err="endpoints is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"endpoints\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Endpoints"
E0731 08:32:31.815597       1 reflector.go:200] "Failed to watch" err="statefulsets.apps is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E0731 08:32:31.815605       1 reflector.go:200] "Failed to watch" err="replicationcontrollers is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E0731 08:32:31.815613       1 reflector.go:200] "Failed to watch" err="ingresses.networking.k8s.io is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"ingresses\" in API group \"networking.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Ingress"
E0731 08:32:31.815626       1 reflector.go:200] "Failed to watch" err="ingressclasses.networking.k8s.io is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"ingressclasses\" in API group \"networking.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.IngressClass"
E0731 08:32:31.815642       1 reflector.go:200] "Failed to watch" err="thanosrulers.monitoring.coreos.com is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"thanosrulers\" in API group \"monitoring.coreos.com\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/metadata/metadatainformer/informer.go:138" type="*v1.PartialObjectMetadata"
E0731 08:32:31.815642       1 reflector.go:200] "Failed to watch" err="resourcequotas is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"resourcequotas\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceQuota"
E0731 08:32:31.815661       1 reflector.go:200] "Failed to watch" err="namespaces is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E0731 08:32:31.815661       1 reflector.go:200] "Failed to watch" err="clusterroles.rbac.authorization.k8s.io is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"clusterroles\" in API group \"rbac.authorization.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ClusterRole"
E0731 08:32:31.815672       1 reflector.go:200] "Failed to watch" err="customresourcedefinitions.apiextensions.k8s.io is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"customresourcedefinitions\" in API group \"apiextensions.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/metadata/metadatainformer/informer.go:138" type="*v1.PartialObjectMetadata"
E0731 08:32:31.815678       1 reflector.go:200] "Failed to watch" err="alertmanagers.monitoring.coreos.com is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"alertmanagers\" in API group \"monitoring.coreos.com\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/metadata/metadatainformer/informer.go:138" type="*v1.PartialObjectMetadata"
E0731 08:32:31.815686       1 reflector.go:200] "Failed to watch" err="prometheuses.monitoring.coreos.com is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"prometheuses\" in API group \"monitoring.coreos.com\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/metadata/metadatainformer/informer.go:138" type="*v1.PartialObjectMetadata"
E0731 08:32:31.815703       1 reflector.go:200] "Failed to watch" err="servicecidrs.networking.k8s.io is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"servicecidrs\" in API group \"networking.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ServiceCIDR"
E0731 08:32:31.815698       1 reflector.go:200] "Failed to watch" err="persistentvolumes is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0731 08:32:31.815718       1 reflector.go:200] "Failed to watch" err="podmonitors.monitoring.coreos.com is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"podmonitors\" in API group \"monitoring.coreos.com\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/metadata/metadatainformer/informer.go:138" type="*v1.PartialObjectMetadata"
E0731 08:32:31.815726       1 reflector.go:200] "Failed to watch" err="horizontalpodautoscalers.autoscaling is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"horizontalpodautoscalers\" in API group \"autoscaling\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v2.HorizontalPodAutoscaler"
E0731 08:32:31.809274       1 reflector.go:200] "Failed to watch" err="volumeattachments.storage.k8s.io is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E0731 08:32:31.810920       1 reflector.go:200] "Failed to watch" err="jobs.batch is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"jobs\" in API group \"batch\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Job"
E0731 08:32:31.815771       1 reflector.go:200] "Failed to watch" err="services is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0731 08:32:31.822628       1 reflector.go:200] "Failed to watch" err="scrapeconfigs.monitoring.coreos.com is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"scrapeconfigs\" in API group \"monitoring.coreos.com\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/metadata/metadatainformer/informer.go:138" type="*v1.PartialObjectMetadata"
E0731 08:32:31.807407       1 reflector.go:200] "Failed to watch" err="secrets is forbidden: User \"system:kube-controller-manager\" cannot watch resource \"secrets\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Secret"
W0731 08:32:31.851199       1 client_builder_dynamic.go:197] get or create service account failed: serviceaccounts "endpointslice-controller" is forbidden: User "system:kube-controller-manager" cannot get resource "serviceaccounts" in API group "" in the namespace "kube-system"
W0731 08:32:31.851355       1 client_builder_dynamic.go:197] get or create service account failed: serviceaccounts "endpoint-controller" is forbidden: User "system:kube-controller-manager" cannot get resource "serviceaccounts" in API group "" in the namespace "kube-system"
E0731 08:32:32.125910       1 event.go:359] "Server rejected event (will not retry!)" err="events is forbidden: User \"system:serviceaccount:kube-system:node-controller\" cannot create resource \"events\" in API group \"\" in the namespace \"default\"" logger="taint-eviction-controller" event="&Event{ObjectMeta:{prometheus-kube-prometheus-operator-d89fb8945-f7n5z.1857484a79571fc4  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Pod,Namespace:default,Name:prometheus-kube-prometheus-operator-d89fb8945-f7n5z,UID:,APIVersion:v1,ResourceVersion:,FieldPath:,},Reason:TaintManagerEviction,Message:Cancelling deletion of Pod default/prometheus-kube-prometheus-operator-d89fb8945-f7n5z,Source:EventSource{Component:taint-eviction-controller,Host:,},FirstTimestamp:2025-07-31 08:21:04.584630212 +0000 UTC m=+7053.646674505,LastTimestamp:2025-07-31 08:21:04.584630212 +0000 UTC m=+7053.646674505,Count:1,Type:Normal,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:taint-eviction-controller,ReportingInstance:,}"
I0731 08:32:46.650667       1 servicecidrs_controller.go:344] "error updating default ServiceCIDR status" logger="service-cidr-controller" error="Unauthorized"
I0731 08:32:52.388548       1 endpointslice_controller.go:344] "Error syncing endpoint slices for service, retrying" logger="endpointslice-controller" key="default/prometheus-kube-prometheus-prometheus" err="EndpointSlice informer cache is out of date"
E0731 08:44:49.850614       1 pv_protection_controller.go:116] "Unhandled Error" err="PV app-pv-02621efd failed with : Operation cannot be fulfilled on persistentvolumes \"app-pv-02621efd\": the object has been modified; please apply your changes to the latest version and try again" logger="UnhandledError"
E0731 08:44:53.786871       1 pvc_protection_controller.go:291] "Error removing protection finalizer from PVC" err="Operation cannot be fulfilled on persistentvolumeclaims \"app-pvc-a95acf9d\": the object has been modified; please apply your changes to the latest version and try again" logger="persistentvolumeclaim-protection-controller" PVC="demo-namespace/app-pvc-a95acf9d"
E0731 08:44:53.789577       1 pvc_protection_controller.go:225] "Unhandled Error" err="PVC app-pvc-a95acf9d/demo-namespace failed with: Operation cannot be fulfilled on persistentvolumeclaims \"app-pvc-a95acf9d\": the object has been modified; please apply your changes to the latest version and try again" logger="UnhandledError"
I0731 09:15:03.692924       1 endpointslice_controller.go:344] "Error syncing endpoint slices for service, retrying" logger="endpointslice-controller" key="default/my-grafana" err="EndpointSlice informer cache is out of date"
I0731 10:02:27.432109       1 endpointslice_controller.go:344] "Error syncing endpoint slices for service, retrying" logger="endpointslice-controller" key="default/prometheus-prometheus-node-exporter" err="EndpointSlice informer cache is out of date"
I0731 10:17:48.138461       1 endpointslice_controller.go:344] "Error syncing endpoint slices for service, retrying" logger="endpointslice-controller" key="default/prometheus-prometheus-node-exporter" err="EndpointSlice informer cache is out of date"
I0731 10:36:06.169927       1 endpointslice_controller.go:344] "Error syncing endpoint slices for service, retrying" logger="endpointslice-controller" key="default/prometheus-kube-prometheus-operator" err="EndpointSlice informer cache is out of date"
